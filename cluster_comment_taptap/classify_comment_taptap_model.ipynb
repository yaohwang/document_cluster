{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-grade",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-topic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FeftvDhjDSPp",
    "outputId": "4ba915de-3a1e-4650-d253-2b43857f2d99"
   },
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_name, mlb):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained(model_name)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, len(mlb.classes_))\n",
    "    \n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1.pooler_output)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-monthly",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KnNeQx6SI78"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-addition",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIEoUm4aQkyl"
   },
   "outputs": [],
   "source": [
    "def validation(model, testing_loader, device, total_batch_test):\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0), total=total_batch_test):\n",
    "            \n",
    "            # X\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            \n",
    "            # y\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            \n",
    "            # pred\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            # TODO:\n",
    "            # fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            # fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "            \n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-dakota",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9_DjWmfWx1q"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, training_loader, testing_loader, epoch, device, total_batch_train, total_batch_test):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_targets = []\n",
    "    epoch_outputs = []\n",
    "    \n",
    "    for _, data in tqdm(enumerate(training_loader, 0), total=total_batch_train): # enumerate(iterable, start=0)\n",
    "        \n",
    "        # _, by batch?\n",
    "        \n",
    "        # X\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        \n",
    "        # y\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        # train\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # optimizer clean\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # if 0 == _ % 5000: print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        epoch_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        epoch_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "        \n",
    "        # TODO: why?\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # optimize\n",
    "        loss.backward() # get new gradient, upon zero grad\n",
    "        optimizer.step()\n",
    "            \n",
    "    loss_train = loss_fn(torch.tensor(epoch_targets), torch.tensor(epoch_outputs)).item()\n",
    "    \n",
    "    fin_outputs, fin_targets = validation(model, testing_loader, device, total_batch_test)\n",
    "    loss_test = loss_fn(torch.tensor(fin_targets), torch.tensor(fin_outputs)).item()\n",
    "    \n",
    "    print(f'epoch: {epoch}, loss train:  {loss_train}, loss test: {loss_test}')\n",
    "    \n",
    "    return model, (loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "speaking-tonight",
   "metadata": {},
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testing_loader, device, total_batch_test, mlb):\n",
    "    \n",
    "    outputs, targets = validation(model, testing_loader, device, total_batch_test)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    \n",
    "    # accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    # f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    # f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    \n",
    "    # TODO:\n",
    "    # print(f\"Accuracy Score = {accuracy}\")\n",
    "    # print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    # print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    \n",
    "    # val_hamming_loss = metrics.hamming_loss(targets, outputs)\n",
    "    # val_hamming_score = hamming_score(np.array(targets), np.array(outputs))\n",
    "\n",
    "    # print(f\"Hamming Score = {val_hamming_score}\")\n",
    "    # print(f\"Hamming Loss = {val_hamming_loss}\")\n",
    "    \n",
    "    print(classification_report(targets, outputs, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-genre",
   "metadata": {},
   "source": [
    "## Ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-bruce",
   "metadata": {},
   "source": [
    "[1] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-nelson",
   "metadata": {},
   "source": [
    "[2] https://huggingface.co/hfl/chinese-bert-wwm <br>\n",
    "[3] https://huggingface.co/hfl/chinese-bert-wwm-ext <br>\n",
    "\n",
    "[4] https://huggingface.co/hfl/chinese-roberta-wwm-ext <br>\n",
    "[5] https://huggingface.co/hfl/chinese-roberta-wwm-ext-large <br>\n",
    "\n",
    "[6] https://huggingface.co/hfl/chinese-macbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-valve",
   "metadata": {},
   "source": [
    "[7] https://huggingface.co/ckiplab/bert-base-chinese <br>\n",
    "[8] https://huggingface.co/ckiplab/albert-tiny-chinese"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
