{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-combination",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.comment_text = dataframe['内容']    # X\n",
    "        self.targets = self.data['标签']    # y(s)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split()) # split, 默认为所有的空字符，包括空格、换行(\\n)、制表符(\\t)等\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            # pad_to_max_length=True,  # @deprecated\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask'] # sentence 有效token位置掩码\n",
    "        token_type_ids = inputs[\"token_type_ids\"] # 多sentence(s)合并为一个sentence时，不同sentence的掩码\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float) # y(s) 没有特别处理\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-archive",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_comments\n",
    "%store -r df_comments_length\n",
    "%store -r df_comments_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-cause",
   "metadata": {},
   "source": [
    "### mu 1std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "mu_1std = df_comments_length.mean(axis=0) + k * df_comments_length.std(axis=0) \n",
    "\n",
    "print(df_comments_length.shape)\n",
    "print(df_comments_length[df_comments_length < mu_1std].shape)\n",
    "print(df_comments_length[df_comments_length < mu_1std].shape[0] / df_comments_length.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-vehicle",
   "metadata": {},
   "source": [
    "### multi label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "# df_comments_topics.loc[:,'标签'] = mlb.fit_transform(df_comments_topics['标签']).tolist()\n",
    "df_comments_topics['标签'] = mlb.fit_transform(df_comments_topics['标签']).tolist()\n",
    "print(df_comments_topics.shape)\n",
    "df_comments_topics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_topics[df_comments_topics['标签'].apply(lambda x: 1 if int == type(x) else len(x)) > 1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "printable-tucson",
   "metadata": {},
   "source": [
    "mlb.classes_.sort()\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "curious-olympus",
   "metadata": {},
   "source": [
    "for i, t in enumerate(mlb.classes_, 1):\n",
    "    print(f'|{i}|{t}|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/mlb.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-laptop",
   "metadata": {},
   "source": [
    "### to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(mu_1std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_comments_topics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "excess-graduate",
   "metadata": {},
   "source": [
    "# TODO: for test only\n",
    "new_df = new_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-durham",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "PkDGqarcPowL",
    "outputId": "e834ea5d-5d00-4bb6-ad52-6f21e744da95"
   },
   "outputs": [],
   "source": [
    "# split train & test\n",
    "\n",
    "train_size = 0.8\n",
    "\n",
    "train_dataset = new_df.sample(frac=train_size, random_state=200)\n",
    "test_dataset = new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "possible-being",
   "metadata": {},
   "source": [
    "test_dataset['标签'] = mlb.inverse_transform(np.array(test_dataset['标签'].to_list()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "marked-oxygen",
   "metadata": {},
   "source": [
    "test_dataset.head(5).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-astronomy",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_dataset\n",
    "%store test_dataset\n",
    "%store mlb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-thumbnail",
   "metadata": {},
   "source": [
    "### api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(model_name, train_dataset, test_dataset, MAX_LEN):\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN) # diff MultiLabelDataset\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "    \n",
    "    return training_set, testing_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
