{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import jieba\n",
    "import pickle\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from itertools import zip_longest\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from optparse import OptionParser\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 10**5)\n",
    "pd.set_option('display.max_colwidth', 10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Option at 0x7f1e53a8a220: --n_features>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = OptionParser()\n",
    "\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\", default=True,\n",
    "              help=\"Print a detailed classification report.\")\n",
    "\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class for every classifier.\")\n",
    "\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Values at 0x7f1e53a8a910: {'print_report': True, 'select_chi2': None, 'print_cm': None, 'print_top10': None, 'use_hashing': None, 'n_features': 65536}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work-around for Jupyter notebook and IPython console\n",
    "\n",
    "# opts, an object containing values for all of your options\n",
    "#     e.g. if --file takes a single string argument, then options.file will be the filename supplied by the user, \n",
    "#     or None if the user did not supply that option\n",
    "# args, the list of positional arguments leftover after parsing options\n",
    "\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --report              Print a detailed classification report.\n",
      "  --chi2_select=SELECT_CHI2\n",
      "                        Select some number of features using a chi-squared\n",
      "                        test\n",
      "  --confusion_matrix    Print the confusion matrix.\n",
      "  --top10               Print ten most discriminative terms per class for\n",
      "                        every classifier.\n",
      "  --use_hashing         Use a hashing vectorizer.\n",
      "  --n_features=N_FEATURES\n",
      "                        n_features when using the hashing vectorizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic(p):\n",
    "    with p.open() as f:\n",
    "        d = json.load(f)\n",
    "        d['标签'] = d['标签'].keys()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rec = namedtuple('record', '游戏名 游戏评分 游戏url 评论数量 标签 用户名 评论时间 游戏评分 游戏时长 内容 手机型号 欢乐 点赞 点踩 回复量')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(d, tag, comment):\n",
    "    comment['标签'] = tag\n",
    "    del comment['回复']\n",
    "    comment.update(d)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(p):\n",
    "    with p.open() as f:\n",
    "        d = json.load(f)\n",
    "        tag_comments = d['标签']\n",
    "        del d['标签']\n",
    "        return [get_comment(d, tag, comment) for tag, comments in tag_comments.items() for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = Path('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('comment.taptap-20210203-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path_root/path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 3.29 s, total: 16.8 s\n",
      "Wall time: 16.8 s\n",
      "(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>游戏名</th>\n",
       "      <th>游戏评分</th>\n",
       "      <th>游戏url</th>\n",
       "      <th>评论数量</th>\n",
       "      <th>标签</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>精灵契约</td>\n",
       "      <td>6.8</td>\n",
       "      <td>https://www.taptap.com/app/142111</td>\n",
       "      <td>870</td>\n",
       "      <td>(过于氪金, 体验不错, 画面优良, 运营不足, 有趣好玩, 玩家互动多, 抽卡概率低, 厂商不给力, 平衡性差, 新手友好, 抄袭嫌疑, web, ios, android, 有游戏时长, 好评, 中评, 差评)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    游戏名 游戏评分                              游戏url  评论数量  \\\n",
       "0  精灵契约  6.8  https://www.taptap.com/app/142111   870   \n",
       "\n",
       "                                                                                                            标签  \n",
       "0  (过于氪金, 体验不错, 画面优良, 运营不足, 有趣好玩, 玩家互动多, 抽卡概率低, 厂商不给力, 平衡性差, 新手友好, 抄袭嫌疑, web, ios, android, 有游戏时长, 好评, 中评, 差评)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_basic = pd.DataFrame([get_basic(p) for p in path.glob('*.json')])\n",
    "print(df_basic.shape)\n",
    "df_basic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:17,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 3.12 s, total: 20.5 s\n",
      "Wall time: 20.5 s\n",
      "(488452, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>游戏评分</th>\n",
       "      <th>游戏时长</th>\n",
       "      <th>内容</th>\n",
       "      <th>手机型号</th>\n",
       "      <th>欢乐</th>\n",
       "      <th>点赞</th>\n",
       "      <th>点踩</th>\n",
       "      <th>回复量</th>\n",
       "      <th>标签</th>\n",
       "      <th>游戏名</th>\n",
       "      <th>游戏url</th>\n",
       "      <th>评论数量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foo云少</td>\n",
       "      <td>2021-01-19 10:07:37</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>不好玩太氪金了我有段时间没有玩号在那个区都不知道了而且这个游戏刚刚出来的时候玩的10区的</td>\n",
       "      <td>华为畅享9 Plus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>过于氪金</td>\n",
       "      <td>精灵契约</td>\n",
       "      <td>https://www.taptap.com/app/142111</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     用户名                 评论时间 游戏评分 游戏时长  \\\n",
       "0  Foo云少  2021-01-19 10:07:37  6.8    0   \n",
       "\n",
       "                                             内容        手机型号  欢乐  点赞  点踩  回复量  \\\n",
       "0  不好玩太氪金了我有段时间没有玩号在那个区都不知道了而且这个游戏刚刚出来的时候玩的10区的  华为畅享9 Plus   0   0   0    0   \n",
       "\n",
       "     标签   游戏名                              游戏url  评论数量  \n",
       "0  过于氪金  精灵契约  https://www.taptap.com/app/142111   870  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_comments = pd.DataFrame(chain.from_iterable([get_comments(p) for p in tqdm(path.glob('*.json'))]))\n",
    "print(df_comments.shape)\n",
    "df_comments.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293435, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>游戏评分</th>\n",
       "      <th>游戏时长</th>\n",
       "      <th>内容</th>\n",
       "      <th>手机型号</th>\n",
       "      <th>欢乐</th>\n",
       "      <th>点赞</th>\n",
       "      <th>点踩</th>\n",
       "      <th>回复量</th>\n",
       "      <th>标签</th>\n",
       "      <th>游戏名</th>\n",
       "      <th>游戏url</th>\n",
       "      <th>评论数量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foo云少</td>\n",
       "      <td>2021-01-19 10:07:37</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>不好玩太氪金了我有段时间没有玩号在那个区都不知道了而且这个游戏刚刚出来的时候玩的10区的</td>\n",
       "      <td>华为畅享9 Plus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>过于氪金</td>\n",
       "      <td>精灵契约</td>\n",
       "      <td>https://www.taptap.com/app/142111</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     用户名                 评论时间 游戏评分 游戏时长  \\\n",
       "0  Foo云少  2021-01-19 10:07:37  6.8    0   \n",
       "\n",
       "                                             内容        手机型号  欢乐  点赞  点踩  回复量  \\\n",
       "0  不好玩太氪金了我有段时间没有玩号在那个区都不知道了而且这个游戏刚刚出来的时候玩的10区的  华为畅享9 Plus   0   0   0    0   \n",
       "\n",
       "     标签   游戏名                              游戏url  评论数量  \n",
       "0  过于氪金  精灵契约  https://www.taptap.com/app/142111   870  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_topic = df_comments[~df_comments.标签.isin(['好评', '中评', '差评'])]\n",
    "print(df_comments_topic.shape)\n",
    "df_comments_topic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.93 s, sys: 92 ms, total: 8.02 s\n",
      "Wall time: 8.01 s\n",
      "(175131, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>内容</th>\n",
       "      <th>标签</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\t  我看大家都没人说魂器，就我上次测试的经验来看，魂器其实也是很重要的，好的魂器搭配上合适的武将，效果绝对是1+1＞2的。\\n官方也会建议一些武将搭配特定属性的魂器，比如奶妈就是回复，菜刀就是物理输出或者连携输出魂器，法师当然就是法器。\\n我觉得有几个魂器是一定要有的：\\n墨家礼装，减少对方输出的，这个基本是队伍辅助必备的，还有月下美人图，强力控制。物理的话，很多了，金箍棒，倚天剑，但是都需要武将来炼化，这个就看你怎么抉择了。哈哈，舍不得孩子套不着狼嘛～如果你搭配的好，真的是有控制，有输出，有回复，能组合出非常完美的搭配，就暂且说这么多吧。</td>\n",
       "      <td>[android]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                    内容  \\\n",
       "0  \\t  我看大家都没人说魂器，就我上次测试的经验来看，魂器其实也是很重要的，好的魂器搭配上合适的武将，效果绝对是1+1＞2的。\\n官方也会建议一些武将搭配特定属性的魂器，比如奶妈就是回复，菜刀就是物理输出或者连携输出魂器，法师当然就是法器。\\n我觉得有几个魂器是一定要有的：\\n墨家礼装，减少对方输出的，这个基本是队伍辅助必备的，还有月下美人图，强力控制。物理的话，很多了，金箍棒，倚天剑，但是都需要武将来炼化，这个就看你怎么抉择了。哈哈，舍不得孩子套不着狼嘛～如果你搭配的好，真的是有控制，有输出，有回复，能组合出非常完美的搭配，就暂且说这么多吧。   \n",
       "\n",
       "          标签  \n",
       "0  [android]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_comments_topics = df_comments_topic[['内容', '标签']].groupby(['内容']).agg(list).reset_index()\n",
    "print(df_comments_topics.shape)\n",
    "df_comments_topics.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'IP还原差',\n",
       " 'UI体验好',\n",
       " 'UI体验差',\n",
       " 'android',\n",
       " 'ios',\n",
       " 'web',\n",
       " '上手难度大',\n",
       " '中评',\n",
       " '优化相关',\n",
       " '体验不错',\n",
       " '体验较差',\n",
       " '值得花钱',\n",
       " '剧情丰富',\n",
       " '剧情单调',\n",
       " '厂商不给力',\n",
       " '厂商良心',\n",
       " '太肝了',\n",
       " '好评',\n",
       " '尊重原著',\n",
       " '差评',\n",
       " '平衡性好',\n",
       " '平衡性差',\n",
       " '广告太多',\n",
       " '广告影响小',\n",
       " '抄袭嫌疑',\n",
       " '护肝',\n",
       " '抽卡概率低',\n",
       " '抽卡概率高',\n",
       " '操作简单',\n",
       " '操作麻烦',\n",
       " '新手友好',\n",
       " '有创新',\n",
       " '有游戏时长',\n",
       " '有趣好玩',\n",
       " '玩家互动多',\n",
       " '玩家互动少',\n",
       " '玩法较差',\n",
       " '画面优良',\n",
       " '画面粗糙',\n",
       " '福利好',\n",
       " '福利差',\n",
       " '自由度低',\n",
       " '自由度高',\n",
       " '过于氪金',\n",
       " '运营不足',\n",
       " '运营给力',\n",
       " '配置要求低',\n",
       " '配置要求高',\n",
       " '音效很棒',\n",
       " '音效较差'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = set(chain.from_iterable(df_basic.标签.apply(list).tolist()))\n",
    "print(len(tags))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182070, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.drop_duplicates('内容').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       105467\n",
       "2        52361\n",
       "3        10861\n",
       "4         3874\n",
       "5         1363\n",
       "6          483\n",
       "7          250\n",
       "8           93\n",
       "9           56\n",
       "13          31\n",
       "10          25\n",
       "11          24\n",
       "15          18\n",
       "12          18\n",
       "16          14\n",
       "14          14\n",
       "17          10\n",
       "18           9\n",
       "20           8\n",
       "21           7\n",
       "19           7\n",
       "25           6\n",
       "22           5\n",
       "29           5\n",
       "28           4\n",
       "24           4\n",
       "31           4\n",
       "33           4\n",
       "26           4\n",
       "44           3\n",
       "36           3\n",
       "43           3\n",
       "32           3\n",
       "46           3\n",
       "47           3\n",
       "49           3\n",
       "23           3\n",
       "30           3\n",
       "53           3\n",
       "52           2\n",
       "117          2\n",
       "100          2\n",
       "60           2\n",
       "42           2\n",
       "40           2\n",
       "27           2\n",
       "38           2\n",
       "35           2\n",
       "277          1\n",
       "267          1\n",
       "263          1\n",
       "139          1\n",
       "153          1\n",
       "142          1\n",
       "151          1\n",
       "655          1\n",
       "272          1\n",
       "152          1\n",
       "2453         1\n",
       "122          1\n",
       "37           1\n",
       "157          1\n",
       "64           1\n",
       "324          1\n",
       "69           1\n",
       "73           1\n",
       "78           1\n",
       "80           1\n",
       "81           1\n",
       "82           1\n",
       "1106         1\n",
       "88           1\n",
       "90           1\n",
       "474          1\n",
       "91           1\n",
       "96           1\n",
       "99           1\n",
       "107          1\n",
       "629          1\n",
       "1142         1\n",
       "68           1\n",
       "63           1\n",
       "287          1\n",
       "702          1\n",
       "34           1\n",
       "121          1\n",
       "1319         1\n",
       "1575         1\n",
       "45           1\n",
       "430          1\n",
       "303          1\n",
       "50           1\n",
       "178          1\n",
       "51           1\n",
       "180          1\n",
       "54           1\n",
       "55           1\n",
       "56           1\n",
       "698          1\n",
       "59           1\n",
       "188          1\n",
       "128          1\n",
       "Name: 标签, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_topics.标签.apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +1/0/-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195017, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>游戏评分</th>\n",
       "      <th>游戏时长</th>\n",
       "      <th>内容</th>\n",
       "      <th>手机型号</th>\n",
       "      <th>欢乐</th>\n",
       "      <th>点赞</th>\n",
       "      <th>点踩</th>\n",
       "      <th>回复量</th>\n",
       "      <th>标签</th>\n",
       "      <th>游戏名</th>\n",
       "      <th>游戏url</th>\n",
       "      <th>评论数量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>白鳞小蛇</td>\n",
       "      <td>2021-01-26 11:37:59</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>还不错，就是太依赖于抽卡，没什么英雄搭配\\r\\n\\r\\n这游戏凉了么？\\r\\n</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>好评</td>\n",
       "      <td>精灵契约</td>\n",
       "      <td>https://www.taptap.com/app/142111</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       用户名                 评论时间 游戏评分 游戏时长  \\\n",
       "1369  白鳞小蛇  2021-01-26 11:37:59  6.8    0   \n",
       "\n",
       "                                           内容 手机型号  欢乐  点赞  点踩  回复量  标签   游戏名  \\\n",
       "1369  还不错，就是太依赖于抽卡，没什么英雄搭配\\r\\n\\r\\n这游戏凉了么？\\r\\n        1   0   0    0  好评  精灵契约   \n",
       "\n",
       "                                  游戏url  评论数量  \n",
       "1369  https://www.taptap.com/app/142111   870  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_pnn = df_comments[df_comments.标签.isin(['好评', '中评', '差评'])]\n",
    "print(df_comments_pnn.shape)\n",
    "df_comments_pnn.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173610, 14)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_pnn.drop_duplicates('内容').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 15:50:59,989 INFO NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "好评    0.591400\n",
       "差评    0.291764\n",
       "中评    0.116836\n",
       "Name: 标签, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_pnn.标签.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train = pd.read_excel('/home/wangyh/project/document_cluster/data/dataset_ads-20210113-1-labeled.xlsx')\n",
    "data_train = data_train[['label', 'content']]\n",
    "data_train = data_train.dropna()\n",
    "\n",
    "print(data_train.shape)\n",
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/wangyh/project/document_cluster/data/dataset_ads-20210120-1-labeled.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1251bc44ce57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/wangyh/project/document_cluster/data/dataset_ads-20210120-1-labeled.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/wangyh/project/document_cluster/data/dataset_ads-20210120-1-labeled.xlsx'"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_excel('/home/wangyh/project/document_cluster/data/dataset_ads-20210120-1-labeled.xlsx')\n",
    "data_test = data_test[['label', 'content']]\n",
    "data_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.loc[data_train.label.isin([1, 9, 24]), 'label'] = -1   # ads\n",
    "data_train.loc[~(data_train.label==-1), 'label'] = 1\n",
    "\n",
    "data_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.loc[data_test.label.isin([1, 9, 24]), 'label'] = -1\n",
    "data_test.loc[~(data_test.label==-1), 'label'] = 1\n",
    "data_test.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = data_train.content, data_test.content\n",
    "y_train, y_test = data_train.label.tolist(), data_test.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_size_mb = size_mb(X_train)\n",
    "data_test_size_mb = size_mb(X_test)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (len(X_train), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (len(X_test), data_test_size_mb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_special(i):\n",
    "    \n",
    "    is_1 =  i in ['<unk>', '<loc>', '<contact>', '<recruit>', '<corpus>', '<colonel>']\n",
    "    is_2 = bool(re.match(r'^<num-[0-9]+>$', i))\n",
    "    \n",
    "    return is_1 or is_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def not_special(i):\n",
    "    return not is_special(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __replace(s):\n",
    "    if is_special(s): return s\n",
    "    \n",
    "    # 微信\n",
    "    s = s.replace('威', '微')\n",
    "    s = s.replace('徽', '微')\n",
    "    s = s.replace('徵', '微')\n",
    "    s = s.replace('亻言', '信')\n",
    "    \n",
    "    s = s.replace('微新', '微信')\n",
    "    s = s.replace('微信', '微')\n",
    "    \n",
    "    # 加\n",
    "    s = s.replace('咖', '加')\n",
    "    s = s.replace('架', '加')\n",
    "    s = s.replace('嫁', '加')\n",
    "    s = s.replace('十', '加')\n",
    "    s = s.replace('茄', '加')\n",
    "    s = s.replace('迦', '加')\n",
    "    \n",
    "    s = s.replace('加下', '加')\n",
    "    s = s.replace('加一下', '加')\n",
    "    \n",
    "    s = s.replace('加', '+')\n",
    "    \n",
    "    # 收人\n",
    "    s = s.replace('活人', '人')\n",
    "    # s = s.replace('收人', '<recruit>')\n",
    "    \n",
    "    # 团长\n",
    "    s = s.replace('圕', '团')\n",
    "    \n",
    "    # 充\n",
    "    s = s.replace('冲', '充')\n",
    "    s = s.replace('直充', '充')\n",
    "    \n",
    "    # 出\n",
    "    s = s.replace('础', '出')\n",
    "\n",
    "    # 卖\n",
    "    s = s.replace('麦', '出')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def replace(x):\n",
    "    return [__replace(i) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split util"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_regex(s, reg, flag):\n",
    "    r = re.split(reg, s)\n",
    "    if 1 >= len(r): return r\n",
    "    r = list(chain.from_iterable(zip_longest(r[:-1], [], fillvalue=flag))) + r[-1:]\n",
    "    return [i for i in r if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split location"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_location(s):\n",
    "    return split_regex(s, r'{localization:[0-9]+\\-[0-9]+}', '<loc>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」{localization:189-393}，889'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}'))\n",
    "print(split_location('{localization:189-393}，88965」'))\n",
    "print(split_location('{localization:189-393}'))\n",
    "print(split_location('2{l4ocalization:189-393}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split terminology"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __split_terminology(s, term, flag):\n",
    "    if is_special(s): return [s]\n",
    "    return split_regex(s, r'%s' % term, flag)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_terminology(x, term, flag):\n",
    "    return list(chain.from_iterable([__split_terminology(i, term, flag) for i in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split coordinates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split num + char"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __split_charnum(s):\n",
    "    if is_special(s): return [s]\n",
    "    return [c for c in re.split(r'([0-9a-z]+)', s) if c]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(__split_charnum('yweighu768980k上jwio880中不为'))\n",
    "print(__split_charnum('上中不为'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_charnum(x):\n",
    "    return list(chain.from_iterable([__split_charnum(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert num"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_numeric(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^0-9]+', s))\n",
    "    \n",
    "    return has_num and hasnot_other"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def convert_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    # return ['<num-%s>' % len(i) if i.isnumeric() else i for i in x]\n",
    "    return ['<num-%s>' % len(i) if not_special(i) and is_numeric(i) else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert num + char"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_charnum(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    has_char = bool(re.findall(r'[a-z]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^a-z0-9]+', s))\n",
    "    \n",
    "    return has_num and has_char and hasnot_other"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_v_num(s):\n",
    "    return bool(re.match(r'v[0-9]+', s))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_vx_num(s):\n",
    "    return bool(re.match(r'vx[0-9]+', s))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def is_qq_num(s):\n",
    "    return bool(re.match(r'qq[0-9]+', s))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __convert_chars_numbers(s):\n",
    "    \n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    if is_v_num(s): return ['微', '<contact>']\n",
    "    if is_vx_num(s): return ['微', '<contact>']\n",
    "    if is_qq_num(s): return ['微', '<contact>']\n",
    "    \n",
    "    if is_charnum(s): return ['<contact>']\n",
    "    \n",
    "    return [s]\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def convert_chars_numbers(x):\n",
    "    # return ['<contact>' if not_special(i) and is_charnum(i) else i for i in x]\n",
    "    return list(chain.from_iterable([__convert_chars_numbers(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split char, num, chinese + special"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s = '◢好◣oooo ┏━卖资源━┓ oooo◢天◣ ◥好◤( 乖)┃高◣┃◢功┃(乖 )◥天◤ ◢游◣ * ( ┃迁◤┃◥勋┃ ) * ◢资◣ ◥戏◤ *__)┗━じovの━┛(__* ◥源◤ __________微信jinyanzifei8__________'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __split_normal_special(s):\n",
    "    if is_special(s): return s, ''\n",
    "    \n",
    "    # TODO: wheather , . ，。blank should be in valid\n",
    "    return ''.join(re.findall(r'[,\\+a-z0-9\\u4e00-\\u9fa5]+', s)), ''.join(re.findall(r'[^,\\+a-z0-9\\u4e00-\\u9fa5]+', s))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_normal_special(x):\n",
    "    \n",
    "    r = [__split_normal_special(i) for i in x]\n",
    "    \n",
    "    r1, r2 = zip(*r)\n",
    "    r1 = [i for i in r1 if i]\n",
    "    r2 = [i for i in r2 if i]\n",
    "    \n",
    "    return r1, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split naive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_naive(s):\n",
    "    # return jieba.lcut(s)\n",
    "    # return jieba.lcut(s, cut_all=True)\n",
    "    # return jieba.lcut_for_search(s)\n",
    "    return list(s)\n",
    "    \n",
    "    # return jieba.lcut(s, cut_all=True) + list(s)\n",
    "    # return jieba.lcut_for_search(s) + list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split once"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __split_once(s):\n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    # s1, s2 = split_normal_special(s)\n",
    "    \n",
    "    # return __split0(s1) + __split0(s2)\n",
    "    # return split_naive(s1) + list(s2)\n",
    "    \n",
    "    # r = split_naive(s1) + list(s)\n",
    "    \n",
    "    # r1 = split_naive(s1)\n",
    "    # r2 = list(s)\n",
    "    # r = r1 + list(set(r2)-set(r1))\n",
    "    \n",
    "    # s = replace(s)\n",
    "    # r = list(s)\n",
    "    r = split_naive(s)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split_once(x):\n",
    "    return list(chain.from_iterable([__split_once(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords1_usual = ['你', '我', '他', '她', '它', '们',\n",
    "                    '吧', '吗', '嘛', '啊', '阿', '呢', '呀',\n",
    "                    '的', '地', \n",
    "                    '怎', '么',\n",
    "                    '那', '哪',\n",
    "                    '就', '没', '了', '谢', '配', '合']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords1 = stopwords1_usual"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords = stopwords1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def filter_stopwords(x):\n",
    "    return [i for i in x if i not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __split(s):\n",
    "    r = []\n",
    "    l = re.split(loc, s)\n",
    "    \n",
    "    for i in l[:-1]:\n",
    "        r.extend(__split1(i))\n",
    "        r.append('<loc>')\n",
    "        \n",
    "    r.extend(__split1(l[-1]))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split1(s):\n",
    "    s = s.lower()\n",
    "    # tokens = jieba.lcut(s, cut_all=True)\n",
    "    # tokens = __split(s)\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # tokens = split_chars_numbers(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    tokens = convert_num(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split1(s):\n",
    "    # preprocess\n",
    "    s = s.replace(' ', '')    # TODO: maybe all blank\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    \n",
    "    # formated\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # normal and special-char\n",
    "    tokens, tokens_special = split_normal_special(tokens)\n",
    "    \n",
    "    # user defined\n",
    "    tokens = split_charnum(tokens)\n",
    "    tokens = convert_num(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    # link\n",
    "    tokens = replace(tokens)\n",
    "    \n",
    "    # split\n",
    "    tokens = split_terminology(tokens, '收人', '<recruit>')\n",
    "    tokens = split_terminology(tokens, '军团', '<corpus>')\n",
    "    tokens = split_terminology(tokens, '团长', '<colonel>')\n",
    "    tokens = split_once(tokens)\n",
    "    \n",
    "    # filter\n",
    "    tokens = filter_stopwords(tokens)\n",
    "    \n",
    "    # merge tokens_special\n",
    "    # tokens += list(''.join(tokens_special))\n",
    "    tokens += ['<special-char>'] * len(''.join(tokens_special))\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(split1('有资源，有功勋 {localization:356-290}'))\n",
    "# print(split1('林昊天 有事找， 你架下他徽 a a z 0 1 2 4'))\n",
    "# print(split1('－－大家看角色名字，需要加ｑ７６６－６４５－８５１菿－付'))\n",
    "print(split1('军团收活人，来的加 1979574312私聊'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "split1 = list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high freq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-2, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def high_freq(x):\n",
    "    return [i for i in x if is_special(i) or (i not in feature_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low freq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-3, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('./model/ads-detect-1-20200125.vocab', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for f in feature_names: print(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def low_freq(x):\n",
    "    return ['<unk>' if i not in feature_names else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def split2(s):\n",
    "    # return low_freq(split1(s))\n",
    "\n",
    "    tokens = split1(s)\n",
    "    # tokens = high_freq(tokens)\n",
    "    tokens = low_freq(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train['tokens'] = data_train['content'].apply(split2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, tokenizer=split2)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(X_test)\n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('./model/ads-detect-1-20200125.emb', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get_support(indices=False)\n",
    "# Get a mask, or integer index, of the features selected\n",
    "#\n",
    "# indicesbool, default=False\n",
    "# If True, the return value will be an array of integers, rather than a boolean mask.\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" % opts.select_chi2)\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    \n",
    "    if feature_names: feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "        \n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if feature_names: feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fit(clf, X_train, y_train):\n",
    "    \n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    \n",
    "    return train_time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict(clf, X_test):\n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    return pred, test_time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def accuracy(y_test, pred):\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def print_coef(clf, opts, feature_names):\n",
    "    # TODO:\n",
    "    \n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            \n",
    "            #for i, label in enumerate(target_names):\n",
    "              #  top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                #print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def print_report(opts, y_test, pred):\n",
    "    \n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def print_cm(opts, y_test, pred):\n",
    "    \n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_desc(clf):\n",
    "    return str(clf).split('(')[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __benchmark(X_train, y_train, X_test, y_test, opts, feature_names, clf):\n",
    "    \n",
    "    train_time = fit(clf, X_train, y_train)\n",
    "    pred, test_time = predict(clf, X_test)\n",
    "    score = accuracy(y_test, pred)\n",
    "    clf_descr = get_desc(clf)\n",
    "\n",
    "    print_coef(clf, opts, feature_names)\n",
    "    print_report(opts, y_test, pred)\n",
    "    print_cm(opts, y_test, pred)\n",
    "    print()\n",
    "    \n",
    "    return pred, clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "benchmark = partial(__benchmark, X_train, y_train, X_test, y_test, opts, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(max_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50, penalty=\"elasticnet\")))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param = {'num_leaves': 2**5-1, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "num_round = 1000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "length = np.expand_dims(data_train['content'].apply(len).to_numpy(), axis=1)\n",
    "print(X_train.shape)\n",
    "print(length.shape)\n",
    "X_train = csr_matrix(np.concatenate((X_train.toarray(), length), axis=1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "length = np.expand_dims(data_test['content'].apply(len).to_numpy(), axis=1)\n",
    "print(X_test.shape)\n",
    "print(length.shape)\n",
    "X_test = csr_matrix(np.concatenate((X_test.toarray(), length), axis=1))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "bst = lgb.train(param, train_data, num_round)\n",
    "\n",
    "y_pred = bst.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bst.save_model('./model/ads-detect-1-20200125.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficiency"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(5)]\n",
    "\n",
    "preds, clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.title(\"Score\")\n",
    "\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\", color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tune"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred, _, _, _, _ = benchmark(ComplementNB(alpha=.1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred, _, _, _, _ = benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_test['pred'] = y_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = data_test[['label', 'pred', 'content']]\n",
    "df['tokens'] = df['content'].apply(split2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df1 = df[(~(df.label==df.pred)) & (df.label==-1)]\n",
    "print(df1.shape)\n",
    "df1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df2 = df[(~(df.label==df.pred)) & (df.label==1)]\n",
    "print(df2.shape)\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
