{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import jieba\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "# from multiprocessing import get_context\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b9ee7a6d8670>:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10**5)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def apply_parallel(df, func, n=-1):\n",
    "    n = os.cpu_count() if -1 == n else n\n",
    "    \n",
    "    df_ = np.array_split(df, n)\n",
    "    with Pool(n) as p:\n",
    "    # with get_context('spawn').Pool(n) as p:\n",
    "        dfr = pd.concat(p.map(func, df_))\n",
    "        # r = p.map(func, df_)\n",
    "    p.join()\n",
    "    \n",
    "    # dfr = pd.Series(chain.from_iterable(r))\n",
    "    # print(df.shape, dfr.shape)\n",
    "    # print(dfr.head(100))\n",
    "    \n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, filename):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path, filename), index_col=0)\n",
    "    # print(df.shape)\n",
    "    # df.head(1)\n",
    "\n",
    "    texts = df[['content']]\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = texts[texts.content == '高价收吴国高迁号，有意的私聊，晚上九点统一回复！']\n",
    "# texts = texts[texts.content == '有意请加VX18526109947']\n",
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts[texts.str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter_localization(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{localization:[0-9]+\\-[0-9]+}$')].reset_index(drop=True)\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_battle(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{battle:[0-9]+,【.*】.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_system_info(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pd.DataFrame(texts.head(10))\n",
    "texts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lower(texts):\n",
    "\n",
    "    texts['tokens'] = texts['content'].str.strip().str.lower()\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_words(texts):\n",
    "\n",
    "    sw = [r'\\s+', r'{localization:[0-9]+\\-[0-9]+}', '丶']\n",
    "\n",
    "    for _ in sw:\n",
    "        texts['tokens'] = texts['tokens'].str.replace(_, '')\n",
    "        # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_only(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].str.isnumeric()]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chars(texts):\n",
    "\n",
    "    dict_merge_chars = {'贝戋' : '贱', '犭句' : '狗', '女马' : '妈'}\n",
    "\n",
    "    for k, v in dict_merge_chars.items():\n",
    "        texts['tokens'] = texts['tokens'].str.replace(k, v)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def tokenize(df):\n",
    "    return df.apply(jieba.lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenization(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(jieba.lcut)\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], jieba.lcut)\n",
    "    # print(len(texts))\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], tokenize)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chars + numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_split_cn(df):\n",
    "    return df.apply(split_chars_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cn(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(split_chars_numbers)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_split_cn)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_split = {\n",
    "'大量出' : ['大量', '出'],\n",
    "'加微信' : ['加', '微信'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_special_words(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda x: list(chain.from_iterable([dict_split.get(i,[i]) for i in x])))\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge2 = [['资源', '商'], ['张', '辽'], ['c', '位'], ['郡', '城'], ['洗洗', '睡'], ['守', '不住'], ['刚', '有事'], ['弃', '坑'], ['新手', '服'], ['带', '兵'], ['老', '样子'], \n",
    "               ['冒个', '泡'], ['白', '嫖'], ['亲', '密度'], ['打寇', '匪'], ['周年', '庆'], ['带', '节奏'], ['玩', '游戏'], ['新', '版本'], ['积点', '德'], ['炸', '矿'], ['赛季', '服'], \n",
    "               ['邺', '城'], ['想', '办法'], ['乌', '骓'], ['龟', '孙'], ['红', '手指'], ['死', '妈'], ['2', '队'], ['等', '会'], ['打', '不过'], ['好', '吧'], ['等', '下'], ['打', '掉'], \n",
    "               ['去', '吧'], ['等', '一下'], ['打', '一下'], ['打', '哪'], ['收兵', '线'], ['撤', '吧'], ['不', '客气'], ['稍', '等'], ['不', '懂'], ['打', '不了'], ['不会', '吧'],\n",
    "              ['不', '稳定'], ['下', '一个'], ['人', '不够'], ['努力', '中'], ['赌', '一把'], ['到', '点'], ['1', '队'], ['没看', '懂'], ['+', '1'], ['合个', '影'], ['燕子', '坞'],\n",
    "              ['一', '梭子']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge3 = [['青山', '不改', '绿水长流'], ['打', '不', '打'], ['来', '不来']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge3(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(1<i):\n",
    "        if [x[i-2],x[i-1],x[i]] in list_merge3:\n",
    "            r.insert(0, ''.join([x[i-2],x[i-1],x[i]]))\n",
    "            i -= 3\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "\n",
    "    if 1 == i:\n",
    "        r.insert(0, x[1])\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge2(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(0<i):\n",
    "        if [x[i-1],x[i]] in list_merge2:\n",
    "            r.insert(0, ''.join([x[i-1],x[i]]))\n",
    "            i -= 2\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "            \n",
    "    if 0 == i:\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_3(df):\n",
    "    return df.apply(merge3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_words_3(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge3)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_3)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_2(df):\n",
    "    return df.apply(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_words_2(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge2)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_2)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_stopwords(path):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            stopwords.extend([w.strip() for w in f.readlines()])            \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords = get_stopwords('/home/wangyh/project/document_cluster/dicts/')\n",
    "print(len(stopwords))\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ads detect only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['…', '⊙', '∀', 'ಡ', 'ω', 'ಡ', '😂', '😜', '谢谢', '哈', '哈哈', '哈哈哈', '哈哈哈哈', '明天', '晚上', '中午', '刷', '刷刷', '霍', '开', '开开', '几点', '没', '改', '军团',\n",
    "     '散', '结束', '切磋', '队伍', '早', '说', '挂免', '丢人', '打匪', '驻防', '感谢', '再见', '来刷', '抱歉', '行军', '飞', '牛', '牛牛', '做人', '善良', '赞', '赞赞', '赞赞赞',\n",
    "     '兄弟', '风云', '文明', '飞过来', '拍', '完', '拉', '求虐', '找', '渣', '喊', '拍照', '试试', '谢', '抢', '发', '没事', '喊', '占', '做', '试', '今晚', '更新', '睡觉', \n",
    "      '走', '帮忙', '帮', '一会', '吃', '╯', '╰', '溜', '偷', '刷不刷', '←', 'ಥ', '加速', '接', '做', '这是', '呱唧', '先打', '县城', '县', '打不动', '不用', '喝酒', '不好意思',\n",
    "     '唧唧', '没事', '速度', '先', '派', '弄', '留个', '纪念', 'ง', '•', '̀', '•', '́', '报名', '新', '新版本', '轮子', '嗲', '女马', '一条', '舔', '菊花', '屁股', '秃顶', '后代', '兔子', \n",
    "     '鹰酱', '全家', '坑货', '合照', '游戏', '太', '团', '里', '诅咒', '合影', '一队', '二队', '二团', '三团', '一群群', '阿猫', '骗', '删', '统一', '签到', '自动', '老子', '说话', \n",
    "      '窝囊废', '死妈', '没妈', '短命', '渣种', '成天', '只会', '亲麻', '打泡', '亲跌', '死', '吠', '骂', '赢', '江湖', '故人', '相逢', '青山', '不改', '绿水长流', '万事', '顺遂',\n",
    "     '尔隆', '咚锵', '进', '差', '远', '又', '，', '都', '是', '就是', '的', '更好', '发展', '利用', '，', '大家', '一起', '作战', '有', '和', '你', '我', '还', '在线', '。', '*', '、', ':',\n",
    "     '您', '能', '为', '了', '就', '（', '？', '吗', '来', '了', '能', '不能', '大', '嘛', '被', '那', '在', '！', '他', '再', '自己', '坐标', '截图', '！', '准备', '退场', '2队', '这', \n",
    "     '积点德', '也', '让', '他们', '啊', '现在', '为了', '你好', '您好', '本', '有志', '青年', '共同', '迎接', '直接', '申请', '蜀国', '加油', '砸种', '狗', '汗间', '魏主', \n",
    "      '祖宗', '现实', '地位', '乐趣', '呢', '认识', '拿', '个', '比', '当', '们', '一场', '持久战', '相信', '胜利', '终将', '属于', '我们', '团结', '一定', '胜利', '欢迎', '所有', \n",
    "      '吴国', '大老', '黑', '又称', '真人', '广大', '群众', '远离', '亲码', '嗷嗷叫', '孝顺', '儿', '懆', '得', '这个', '喔', '猪', '拱', '浑身', '溃烂', '正在', '喝', '脓血', '疗伤',\n",
    "     '可怜', '家里', '得', '剩', '知道', '不', '容易', '父母', '等', '着', '拿', '钱', '买', '棺材', '下葬', '呢', '可是', '那点', '父母', '烧纸', '不够',\n",
    "      '不', '知道', '回', '云天', '陪', '所谓', '“', '”', '还是', '像', '癞皮狗', '一样', '继续', '留在', '大腿', '可以', '抱', '团里', '当奴才', '拭目以待',\n",
    "     '种地', '现实', '残酷', '农民', '们', '辛苦', '啦', '不辞辛苦', '坚持', '帽子', '小', '农民', '思维', '真是', '单纯', '可爱', '惹', '人', '疼', '呢', '不', '知道', '不', '打', \n",
    "      '宝服', '傻', '笔王', '人',  '呢', '打', '吧', '此', '不肖', '子', '还有', '人', '集结', '留念', '最后', '一期', '好', '凑个', '脸熟', '什么', '意思', '嗯', '好', '情况', '二货',\n",
    "     '今天', '敌人', '朋友', '这里', '打', '可能', '延迟', '几分钟', '尽量', '保证', '每', '一位', '这个', '时候', '呵呵', '呵', '好', '嘞', '恩怨', '一笑', '泯', '恩仇',\n",
    "     '啥', '情况', '你们', '哦', '好', '顶战', '清人', '(', ')', '刚', '上线', '我们', '加油', '成员', '全体', '集合', '民心', '任务', '吧', '哪', '认识', '清楚', '么', '分钟', '打不过', \n",
    "     '别', '呀', '战力', '以后', '再升', '留点', '兵', '去', '有人', '土匪', '来个', '帮帮忙', '：', '）', '有人', '碰瓷', '厉害', '呀', '怎么', '不理', '因为', '觉得', '咋样',\n",
    "      '慢慢', '玩吧', '亏', '大发', '别', '互相', '伤害', '告诉', '打野', '有点', '过分', '回家', '公告', '看', '去', '将', '我开', '相忘', '于', '到', '旁边', '乐', '有缘千里', '相会',\n",
    "     '有缘', '各位', '真', '无聊', '分钟', '我来', '看看', '揍', '没有', '一下', '一个', '上', '下', '用', '不要', '位置', '这么', '繁荣', '谁', '玩', '秒', '把', '多少', '多', '魏国',\n",
    "      '一二三四五', '过来', '九点', '长安', '次', '不是', '开始', '阵容', '以上', '时间', '很', '清', '带', '城', '打飞', '会', '咯', '地点', '郡城', '留影', '洛阳', '想', '有没有', \n",
    "      '搞', '祝', '提前', '随便', '看见', '个人', '起来', '一炮', '不行', '叫', '对', '落位', '上车', '天天', '不好', '令', '分', '区', '高', '王', '其他', '通告', '围城', '联盟',\n",
    "     '快', '兵线', '地方', '够', '咋', '线', '小时', '跑', '后面', '无', '起', '那个', '蜀', '散人', '郡', '所有人', '本人', '原始', '娱乐', '赶紧', '一点', 'c位', '错', '建筑',\n",
    "     '成都', '一环', '凑', '希望', '撤', '飞过去', '配合', '其他人', '过', '出兵', '那么', '随机', '最好', '全部', '才', '一直', '领', '中', ',', '可', '现', '开心', '跟', '主公',\n",
    "     '炸', '马上', '本服', '照', '过节', '大小', '报仇', '三国', '今后', '以前', '消', '弟兄们', '早到', '已经', '自立', '城池', '准时', '种田', '春节', '已放', '下位', '处',\n",
    "     '方向', '参加', '该', '浪费', '照片', '世界', '见', '一次', '集体', '做个', '到位', '差距', '天', '就位', '打城', '没出息', '奇怪', '好像', '坟头', '不了', '哪个', '不到', \n",
    "     '没兵', '彻底', '报', '许昌', '哥哥', '刚刚', '再来', '刚才', '回来', '如果', '出来', '几次', '总是', '应该', '好多', '近', '长沙', '一兵', '不来', '太虚', '随意', '只有',\n",
    "     '等会', '条渣种', '此时此刻', '被控', '第一', '深更半夜', '平民', '霸服', '脸', '本事', '打出', '别说', '控号', '有钱', '别人', '被控', '残', '狗东西', \n",
    "      '地儿', '隆冬', '强', '咚咚', '锵', '整个', '挨个', '群', 'bb', '笔', '内', '间', '内', '畜生', '字', '小名', '童子', '喜', '男人', '男风', '者', '不怕', 'py',\n",
    "     '狗奴才', '嘴上', '立人', '设时', '连', '当真', '但', '经不起', '任何', '考验', '啪啪', '打脸', '两面三刀', '小人', '晚', '点', '自由', '开放', '1队', '平安', '喜乐', '月',\n",
    "     '解散', '麻烦', '随迁', '东瀛', '鬼子', '野种', '全飞', '保持', '队形', '寒江', '孤影', '何必', '曾', '相识', '睡', '晚安', '手机', '没电', '123', '茄子', '收兵线', '升', \n",
    "      '进步', '很大', '满级', '重', '再也', '不想', '欺负', '弱团', '欺负', '本国', '弱小', '分手', '打掉', '行', '散伙', '臭表子', '一组', '过去', '掉', '全一环', '格子',\n",
    "     '校场', '副', '再也', '不见', '吓', '跳', '傻笔', '每天', '早点', '休息', '称号', '废', '八姓', '家奴', '几把', '恶是', '几把',\n",
    "     '赛季', '服前', '一天', '一路', '涪陵', '有空', '按', '顺序', '整体', '排列', '点会', '安排', '大殿', '不带', '协防', '好友', '徒弟', '小车', '死光光',\n",
    "     '明晚', '长安城', '本区', '提供', '具体地址', '时发', '城到', '铁血', '具体', '吊打', '录屏', '注意', '有刷', '暂时', '离团', '南门', '军旗', '贱狗', '爽', '狗娘养',\n",
    "    '西南', '门', '军旗', '围', '三环', '抓紧', '骑兵', '进攻',  '抢新', '版本', '首站', '天王', '盖地', '虎', '大白', '卖', '蓝色', '蜀汉', '旗', '大白', '告', '妻',\n",
    "     '银河', '奥特曼', '沙比', '总喊', '玛', '没钱别', '比比', '能耐', '纳', '闷', '不多', '鱼名', '各路', '英雄', '叛贼', '打家劫舍', '原', '区不分', '国籍', '拍个',\n",
    "     '钱大狗', '期待', '与', '再次', '相遇', '期', '截止', '想来', '飞行', '玩耍', '万一', '低战图']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = list(set(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.extend(sw)\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def filter_sw(df):\n",
    "    \n",
    "    # return pd.Series([w for w in x if w not in stopwords])\n",
    "    \n",
    "    # for w in x:\n",
    "       # print(w, w not in stopwords)\n",
    "    # return [w for w in x if w not in stopwords]\n",
    "    \n",
    "    return df.apply(lambda l: [w for w in l if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda l: [w for w in l if w not in stopwords])\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], lambda l: [w for w in l if w not in stopwords])\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], filter_sw)\n",
    "    # texts = texts.dropna()\n",
    "    \n",
    "    \n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    # print(x)\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    if 2 == len(x): \n",
    "        return [x[0]] if x[0] == x[1] else x\n",
    "    \n",
    "    r = [x[i] for i in range(len(x)-2) if x[i] != x[i+1]]\n",
    "    if x[-1] == x[-2]:\n",
    "        r.append(x[-2])\n",
    "    else:\n",
    "        r.extend(x[-2:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_repeats(texts):\n",
    "\n",
    "    # 谢谢, 谢谢\n",
    "    # 喝酒, 喝酒\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(remove_repeats)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'v' + num\n",
    "\n",
    "def is_v_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and 'v' == x[0] and x[1].isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_v_num(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_v_num)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num + '级'\n",
    "\n",
    "def is_num_ji(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and x[0].isnumeric() and '级' == x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_ji(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_num_ji)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(texts):\n",
    "    # texts['length'] = texts['tokens'].apply(len)\n",
    "    \n",
    "    texts = texts[texts['tokens'].apply(len) > 1]\n",
    "    # print(len(texts))\n",
    "    # print(texts.tail(10))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# df = pd.DataFrame(texts)\n",
    "# df.head(1)\n",
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(texts):\n",
    "\n",
    "    # df = df.astype(str).value_counts().reset_index().rename(columns={0:'cnt'})\n",
    "    # df.head(1)\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].astype(str)\n",
    "    df = texts.groupby('tokens').agg({'tokens':['count'], 'content':[lambda x: pd.DataFrame.head(x,1)]}).reset_index()\n",
    "\n",
    "    df.columns = ['tokens', 'cnt', 'content']\n",
    "    df = df[['tokens','content', 'cnt']]\n",
    "    df = df.sort_values('cnt', ascending=False).reset_index(drop=True)\n",
    "    # df.head(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freq more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_more(df, n):\n",
    "\n",
    "    dfn = df[df['cnt'] >= n]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_between(df, n1, n2):\n",
    "\n",
    "    dfn = df[(df['cnt'] >= n1) & (df['cnt'] < n2)]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df5 = df[df['cnt'] >= 5]\n",
    "print(df5.shape)\n",
    "df5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df4 = df[(df['cnt'] < 5) & (df['cnt'] >= 4)]\n",
    "print(df4.shape)\n",
    "df4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df3 = df[(df['cnt'] < 4) & (df['cnt'] >= 3)]\n",
    "print(df3.shape)\n",
    "df3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for tokens in df3.tokens:\n",
    "    for t in set(eval(tokens)):\n",
    "        counter[t] += 1\n",
    "        \n",
    "counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df[(df['cnt'] < 3) & (df['cnt'] >= 2)]\n",
    "print(df2.shape)\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfn = df[df['cnt'] > 1]\n",
    "print(dfn.shape)\n",
    "# dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df1 = df[df['cnt'] <= 1]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2期 标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracts(path, filename):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        texts = load(path, filename)\n",
    "        # texts = texts.head(10)\n",
    "\n",
    "        pipeline = [\n",
    "            filter_localization, \n",
    "            filter_battle,\n",
    "            filter_system_info,\n",
    "\n",
    "            clean_lower,\n",
    "            clean_special_words,\n",
    "            clean_numeric_only,\n",
    "            merge_chars,\n",
    "\n",
    "            tokenization,\n",
    "            split_cn,\n",
    "            split_special_words,\n",
    "            merge_words_3,\n",
    "            merge_words_2,\n",
    "            filter_stopwords,\n",
    "            filter_repeats,\n",
    "\n",
    "            filter_v_num,\n",
    "            filter_num_ji,\n",
    "            filter_length,\n",
    "                   ]\n",
    "\n",
    "        for f in pipeline:\n",
    "            # print(f)\n",
    "            t0 = time.time()\n",
    "            texts = f(texts)\n",
    "            # print(time.time()-t0)\n",
    "\n",
    "        df = extract(texts)\n",
    "\n",
    "        # dfn = freq_more(df, 5)\n",
    "        dfn = freq_between(df, 3, 5)\n",
    "        print(dfn.shape)\n",
    "\n",
    "        return dfn\n",
    "    \n",
    "    except:\n",
    "        print(filename)\n",
    "        traceback.print_exc()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/wangyh/data/yk-sgz2017-chat/data-2-20201223'\n",
    "# filename = '2020_09_25.csv'\n",
    "# filename = '2020_09_24.csv'\n",
    "# filename = '2020_08_20.csv'\n",
    "\n",
    "path = '/home/wangyh/data/yk-sgz2017-chat/data-7-20210120'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time extracts(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.203 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.201 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.200 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.194 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.208 seconds.\n",
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.201 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.240 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 3)\n",
      "(10, 3)\n",
      "(19, 3)\n",
      "(25, 3)\n",
      "(29, 3)\n",
      "(20, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(22, 3)\n",
      "(12, 3)\n",
      "(9, 3)\n",
      "(16, 3)\n",
      "(25, 3)\n",
      "(26, 3)\n",
      "(16, 3)\n",
      "(8, 3)\n",
      "(20, 3)\n",
      "(9, 3)\n",
      "(22, 3)\n",
      "(69, 3)\n",
      "(7, 3)\n",
      "(31, 3)\n",
      "(50, 3)\n",
      "(8, 3)\n",
      "(27, 3)\n",
      "(35, 3)\n",
      "(23, 3)\n",
      "(61, 3)\n",
      "(5, 3)\n",
      "(64, 3)\n",
      "(39, 3)\n",
      "(28, 3)\n",
      "(21, 3)\n",
      "(14, 3)\n",
      "(29, 3)\n",
      "(24, 3)\n",
      "(8, 3)\n",
      "(23, 3)\n",
      "(40, 3)\n",
      "(13, 3)\n",
      "(17, 3)\n",
      "(11, 3)\n",
      "(11, 3)\n",
      "(26, 3)\n",
      "(40, 3)\n",
      "(14, 3)\n",
      "(9, 3)\n",
      "(21, 3)\n",
      "(32, 3)\n",
      "(40, 3)\n",
      "(19, 3)\n",
      "(6, 3)\n",
      "(24, 3)\n",
      "(18, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(24, 3)\n",
      "(18, 3)\n",
      "(11, 3)\n",
      "(12, 3)\n",
      "(20, 3)\n",
      "(24, 3)\n",
      "(10, 3)\n",
      "(13, 3)\n",
      "(10, 3)\n",
      "(19, 3)\n",
      "(18, 3)\n",
      "(19, 3)\n",
      "(19, 3)\n",
      "(31, 3)\n",
      "(16, 3)\n",
      "(13, 3)\n",
      "(7, 3)\n",
      "(34, 3)\n",
      "(20, 3)\n",
      "(27, 3)\n",
      "(8, 3)\n",
      "(13, 3)\n",
      "(8, 3)\n",
      "(23, 3)\n",
      "(21, 3)\n",
      "(17, 3)\n",
      "(17, 3)\n",
      "(9, 3)\n",
      "(19, 3)\n",
      "(19, 3)\n",
      "(26, 3)\n",
      "(14, 3)\n",
      "(30, 3)\n",
      "(15, 3)\n",
      "(21, 3)\n",
      "(7, 3)\n",
      "(18, 3)\n",
      "(25, 3)\n",
      "(20, 3)\n",
      "(6, 3)\n",
      "(14, 3)\n",
      "(4, 3)\n",
      "(22, 3)\n",
      "(32, 3)\n",
      "(21, 3)\n",
      "(20, 3)\n",
      "(25, 3)\n",
      "(5, 3)\n",
      "(37, 3)\n",
      "(10, 3)\n",
      "109.33714628219604\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(path)))\n",
    "t0 = time.time()\n",
    "# dfs = [extracts(path, filename) for filename in tqdm(os.listdir(path))]\n",
    "\n",
    "with Pool(8) as p:\n",
    "    dfs = p.map(partial(extracts, path), os.listdir(path))\n",
    "    # filenames = os.listdir(path)\n",
    "    # for _ in tqdm(p.map(partial(extracts, path), filenames), total=len(filenames)): pass\n",
    "p.join()\n",
    "\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1453, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['第', '12', '章', '第', '5', '关', '副本', 'n', '背强', '退', 'n', '鬼']</td>\n",
       "      <td>第12章第5关副本打N次背强退N次，搞什么鬼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['加', '微信', '进群']</td>\n",
       "      <td>加我微信拉你进群</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['挂', '免战']</td>\n",
       "      <td>挂免战了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['的话', '加', '微信', '进群']</td>\n",
       "      <td>你去的话加我微信，我拉你进群</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['首战', '奖', '10', '微信', '下一城', '留兵', '留', '元宝', '投石机', '资源', '30', '捐', '宜威', '捐到', '七级']</td>\n",
       "      <td>今晚首战奖10点微信集合！下一城！留兵留元宝！有投石机！资源每天30次捐宜威郡捐到七级。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['留兵', '留令', '资源', '该升', '升级', '没捐城', '捐', '宜威', '10', '首战']</td>\n",
       "      <td>留兵留令资源该升的升级，没捐城的捐宜威！10点集合！首战！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['sg', '888']</td>\n",
       "      <td>sg888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['收号', '私聊']</td>\n",
       "      <td>收号私聊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['w', '18370951233']</td>\n",
       "      <td>w18370951233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['扣', '1']</td>\n",
       "      <td>一个是 扣1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      tokens  \\\n",
       "0  ['第', '12', '章', '第', '5', '关', '副本', 'n', '背强', '退', 'n', '鬼']                             \n",
       "1  ['加', '微信', '进群']                                                                           \n",
       "2  ['挂', '免战']                                                                                 \n",
       "3  ['的话', '加', '微信', '进群']                                                                     \n",
       "4  ['首战', '奖', '10', '微信', '下一城', '留兵', '留', '元宝', '投石机', '资源', '30', '捐', '宜威', '捐到', '七级']   \n",
       "5  ['留兵', '留令', '资源', '该升', '升级', '没捐城', '捐', '宜威', '10', '首战']                                \n",
       "6  ['sg', '888']                                                                               \n",
       "7  ['收号', '私聊']                                                                                \n",
       "8  ['w', '18370951233']                                                                        \n",
       "9  ['扣', '1']                                                                                  \n",
       "\n",
       "                                        content  \n",
       "0  第12章第5关副本打N次背强退N次，搞什么鬼                        \n",
       "1  加我微信拉你进群                                      \n",
       "2  挂免战了                                          \n",
       "3  你去的话加我微信，我拉你进群                                \n",
       "4  今晚首战奖10点微信集合！下一城！留兵留元宝！有投石机！资源每天30次捐宜威郡捐到七级。  \n",
       "5  留兵留令资源该升的升级，没捐城的捐宜威！10点集合！首战！                 \n",
       "6  sg888                                         \n",
       "7  收号私聊                                          \n",
       "8  w18370951233                                  \n",
       "9  一个是 扣1                                        "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df_final.drop(columns=['cnt'])\n",
    "df_ = df_.drop_duplicates('tokens')\n",
    "# df_.to_excel('./data/dataset_ads-20210113-1.xlsx', encoding='utf8', index=False)\n",
    "# df_.to_excel('./data/dataset_ads-20210120-1.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "# df_.to_excel('./data/dataset_ads-20210201-1.xlsx', encoding='utf8', index=False)\n",
    "df_.to_excel('./data/dataset_ads-20210201-2.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "print(df_.shape)\n",
    "df_.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
