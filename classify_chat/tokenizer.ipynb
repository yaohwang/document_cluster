{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from itertools import chain\n",
    "from itertools import zip_longest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-speaker",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_train\n",
    "%store -r X_train\n",
    "%store -r data_train_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-testing",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_special(i):\n",
    "    \n",
    "    is_1 =  i in ['<unk>', '<loc>', '<contact>', '<recruit>', '<corpus>', '<colonel>']\n",
    "    is_2 = bool(re.match(r'^<num-[0-9]+>$', i))\n",
    "    \n",
    "    return is_1 or is_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_special(i):\n",
    "    return not is_special(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-azerbaijan",
   "metadata": {},
   "source": [
    "### replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __replace(s):\n",
    "    if is_special(s): return s\n",
    "    \n",
    "    # 微信\n",
    "    s = s.replace('威', '微')\n",
    "    s = s.replace('徽', '微')\n",
    "    s = s.replace('徵', '微')\n",
    "    s = s.replace('亻言', '信')\n",
    "    \n",
    "    s = s.replace('微新', '微信')\n",
    "    s = s.replace('微信', '微')\n",
    "    \n",
    "    # 加\n",
    "    s = s.replace('咖', '加')\n",
    "    s = s.replace('架', '加')\n",
    "    s = s.replace('嫁', '加')\n",
    "    s = s.replace('十', '加')\n",
    "    s = s.replace('茄', '加')\n",
    "    s = s.replace('迦', '加')\n",
    "    \n",
    "    s = s.replace('加下', '加')\n",
    "    s = s.replace('加一下', '加')\n",
    "    \n",
    "    s = s.replace('加', '+')\n",
    "    \n",
    "    # 收人\n",
    "    s = s.replace('活人', '人')\n",
    "    # s = s.replace('收人', '<recruit>')\n",
    "    \n",
    "    # 团长\n",
    "    s = s.replace('圕', '团')\n",
    "    \n",
    "    # 充\n",
    "    s = s.replace('冲', '充')\n",
    "    s = s.replace('直充', '充')\n",
    "    \n",
    "    # 出\n",
    "    s = s.replace('础', '出')\n",
    "\n",
    "    # 卖\n",
    "    s = s.replace('麦', '出')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(x):\n",
    "    return [__replace(i) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-freeze",
   "metadata": {},
   "source": [
    "### split util"
   ]
  },
  {
   "cell_type": "raw",
   "id": "offensive-words",
   "metadata": {},
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "helpful-assessment",
   "metadata": {},
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_regex(s, reg, flag):\n",
    "    r = re.split(reg, s)\n",
    "    if 1 >= len(r): return r\n",
    "    r = list(chain.from_iterable(zip_longest(r[:-1], [], fillvalue=flag))) + r[-1:]\n",
    "    return [i for i in r if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-catholic",
   "metadata": {},
   "source": [
    "#### split location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(s):\n",
    "    return split_regex(s, r'{localization:[0-9]+\\-[0-9]+}', '<loc>')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "senior-waterproof",
   "metadata": {},
   "source": [
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」{localization:189-393}，889'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}'))\n",
    "print(split_location('{localization:189-393}，88965」'))\n",
    "print(split_location('{localization:189-393}'))\n",
    "print(split_location('2{l4ocalization:189-393}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-cruise",
   "metadata": {},
   "source": [
    "#### split terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_terminology(s, term, flag):\n",
    "    if is_special(s): return [s]\n",
    "    return split_regex(s, r'%s' % term, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_terminology(x, term, flag):\n",
    "    return list(chain.from_iterable([__split_terminology(i, term, flag) for i in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-observer",
   "metadata": {},
   "source": [
    "#### split coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-necessity",
   "metadata": {},
   "source": [
    "#### split num + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_charnum(s):\n",
    "    if is_special(s): return [s]\n",
    "    return [c for c in re.split(r'([0-9a-z]+)', s) if c]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "african-parts",
   "metadata": {},
   "source": [
    "print(__split_charnum('yweighu768980k上jwio880中不为'))\n",
    "print(__split_charnum('上中不为'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_charnum(x):\n",
    "    return list(chain.from_iterable([__split_charnum(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-laptop",
   "metadata": {},
   "source": [
    "#### convert num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^0-9]+', s))\n",
    "    \n",
    "    return has_num and hasnot_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    # return ['<num-%s>' % len(i) if i.isnumeric() else i for i in x]\n",
    "    return ['<num-%s>' % len(i) if not_special(i) and is_numeric(i) else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-adelaide",
   "metadata": {},
   "source": [
    "#### convert num + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_charnum(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    has_char = bool(re.findall(r'[a-z]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^a-z0-9]+', s))\n",
    "    \n",
    "    return has_num and has_char and hasnot_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_v_num(s):\n",
    "    return bool(re.match(r'v[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vx_num(s):\n",
    "    return bool(re.match(r'vx[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_qq_num(s):\n",
    "    return bool(re.match(r'qq[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __convert_chars_numbers(s):\n",
    "    \n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    if is_v_num(s): return ['微', '<contact>']\n",
    "    if is_vx_num(s): return ['微', '<contact>']\n",
    "    if is_qq_num(s): return ['微', '<contact>']\n",
    "    \n",
    "    if is_charnum(s): return ['<contact>']\n",
    "    \n",
    "    return [s]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_chars_numbers(x):\n",
    "    # return ['<contact>' if not_special(i) and is_charnum(i) else i for i in x]\n",
    "    return list(chain.from_iterable([__convert_chars_numbers(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-hygiene",
   "metadata": {},
   "source": [
    "#### split char, num, chinese + special"
   ]
  },
  {
   "cell_type": "raw",
   "id": "appreciated-superior",
   "metadata": {},
   "source": [
    "s = '◢好◣oooo ┏━卖资源━┓ oooo◢天◣ ◥好◤( 乖)┃高◣┃◢功┃(乖 )◥天◤ ◢游◣ * ( ┃迁◤┃◥勋┃ ) * ◢资◣ ◥戏◤ *__)┗━じovの━┛(__* ◥源◤ __________微信jinyanzifei8__________'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_normal_special(s):\n",
    "    if is_special(s): return s, ''\n",
    "    \n",
    "    # TODO: wheather , . ，。blank should be in valid\n",
    "    return ''.join(re.findall(r'[,\\+a-z0-9\\u4e00-\\u9fa5]+', s)), ''.join(re.findall(r'[^,\\+a-z0-9\\u4e00-\\u9fa5]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_normal_special(x):\n",
    "    \n",
    "    r = [__split_normal_special(i) for i in x]\n",
    "    \n",
    "    r1, r2 = zip(*r)\n",
    "    r1 = [i for i in r1 if i]\n",
    "    r2 = [i for i in r2 if i]\n",
    "    \n",
    "    return r1, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-lover",
   "metadata": {},
   "source": [
    "#### split naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_naive(s):\n",
    "    # return jieba.lcut(s)\n",
    "    # return jieba.lcut(s, cut_all=True)\n",
    "    # return jieba.lcut_for_search(s)\n",
    "    return list(s)\n",
    "    \n",
    "    # return jieba.lcut(s, cut_all=True) + list(s)\n",
    "    # return jieba.lcut_for_search(s) + list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-easter",
   "metadata": {},
   "source": [
    "#### split once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_once(s):\n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    # s1, s2 = split_normal_special(s)\n",
    "    \n",
    "    # return __split0(s1) + __split0(s2)\n",
    "    # return split_naive(s1) + list(s2)\n",
    "    \n",
    "    # r = split_naive(s1) + list(s)\n",
    "    \n",
    "    # r1 = split_naive(s1)\n",
    "    # r2 = list(s)\n",
    "    # r = r1 + list(set(r2)-set(r1))\n",
    "    \n",
    "    # s = replace(s)\n",
    "    # r = list(s)\n",
    "    r = split_naive(s)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_once(x):\n",
    "    return list(chain.from_iterable([__split_once(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-penetration",
   "metadata": {},
   "source": [
    "#### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1_usual = ['你', '我', '他', '她', '它', '们',\n",
    "                    '吧', '吗', '嘛', '啊', '阿', '呢', '呀',\n",
    "                    '的', '地', \n",
    "                    '怎', '么',\n",
    "                    '那', '哪',\n",
    "                    '就', '没', '了', '谢', '配', '合']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1 = stopwords1_usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(x):\n",
    "    return [i for i in x if i not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-vessel",
   "metadata": {},
   "source": [
    "### split 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "anonymous-journalism",
   "metadata": {},
   "source": [
    "def __split(s):\n",
    "    r = []\n",
    "    l = re.split(loc, s)\n",
    "    \n",
    "    for i in l[:-1]:\n",
    "        r.extend(__split1(i))\n",
    "        r.append('<loc>')\n",
    "        \n",
    "    r.extend(__split1(l[-1]))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "major-generation",
   "metadata": {},
   "source": [
    "def split1(s):\n",
    "    s = s.lower()\n",
    "    # tokens = jieba.lcut(s, cut_all=True)\n",
    "    # tokens = __split(s)\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # tokens = split_chars_numbers(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    tokens = convert_num(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split1(s):\n",
    "    # preprocess\n",
    "    s = s.replace(' ', '')    # TODO: maybe all blank\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    \n",
    "    # formated\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # normal and special-char\n",
    "    tokens, tokens_special = split_normal_special(tokens)\n",
    "    \n",
    "    # user defined\n",
    "    tokens = split_charnum(tokens)\n",
    "    tokens = convert_num(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    # link\n",
    "    tokens = replace(tokens)\n",
    "    \n",
    "    # split\n",
    "    tokens = split_terminology(tokens, '收人', '<recruit>')\n",
    "    tokens = split_terminology(tokens, '军团', '<corpus>')\n",
    "    tokens = split_terminology(tokens, '团长', '<colonel>')\n",
    "    tokens = split_once(tokens)\n",
    "    \n",
    "    # filter\n",
    "    tokens = filter_stopwords(tokens)\n",
    "    \n",
    "    # merge tokens_special\n",
    "    # tokens += list(''.join(tokens_special))\n",
    "    tokens += ['<special-char>'] * len(''.join(tokens_special))\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "surface-newton",
   "metadata": {},
   "source": [
    "# print(split1('有资源，有功勋 {localization:356-290}'))\n",
    "# print(split1('林昊天 有事找， 你架下他徽 a a z 0 1 2 4'))\n",
    "# print(split1('－－大家看角色名字，需要加ｑ７６６－６４５－８５１菿－付'))\n",
    "print(split1('军团收活人，来的加 1979574312私聊'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "another-sunset",
   "metadata": {},
   "source": [
    "split1 = list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-trainer",
   "metadata": {},
   "source": [
    "### high freq"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bridal-buddy",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-2, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "smoking-cleanup",
   "metadata": {},
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "id": "elegant-nerve",
   "metadata": {},
   "source": [
    "def high_freq(x):\n",
    "    return [i for i in x if is_special(i) or (i not in feature_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-livestock",
   "metadata": {},
   "source": [
    "### low freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-3, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/ads-detect-1-20200125.vocab', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "iraqi-facing",
   "metadata": {},
   "source": [
    "for f in feature_names: print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_freq(x):\n",
    "    return ['<unk>' if i not in feature_names else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-score",
   "metadata": {},
   "source": [
    "### split 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2(s):\n",
    "    # return low_freq(split1(s))\n",
    "\n",
    "    tokens = split1(s)\n",
    "    # tokens = high_freq(tokens)\n",
    "    tokens = low_freq(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-short",
   "metadata": {},
   "source": [
    "### split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['tokens'] = data_train['content'].apply(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
