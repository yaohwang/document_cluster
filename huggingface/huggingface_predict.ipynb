{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-apollo",
   "metadata": {},
   "source": [
    "Using exists models directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-algorithm",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. how to train model;\n",
    "2. how to tune on pretrained model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aboriginal-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lasting-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-instrumentation",
   "metadata": {},
   "source": [
    "## AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-export",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "robust-radiation",
   "metadata": {},
   "source": [
    "# OOM\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bridal-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not found\n",
    "# only pyTorch model exists, need pyTorch library installed\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arranged-ballet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7725350856781006}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tender-contamination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7725348472595215},\n",
       " {'label': '5 stars', 'score': 0.2365245521068573}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "           \"We hope you don't hate it.\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sorted-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5 stars, with score: 0.7725\n",
      "label: 5 stars, with score: 0.2365\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-sister",
   "metadata": {},
   "source": [
    "### Pipeline: Tokenizer + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "emerging-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "divine-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weird-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parliamentary-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-factory",
   "metadata": {},
   "source": [
    "### Tokenizer + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadly-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-special",
   "metadata": {},
   "source": [
    "#### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proper-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disabled-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rotary-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\", \"Hellow world!\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "strange-favorite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0], [101, 7592, 2860, 2088, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tf_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-tuesday",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "engaging-bedroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_57']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-netscape",
   "metadata": {},
   "source": [
    "#### predict activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "civil-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-4.0832963 ,  4.336415  ],\n",
       "       [ 0.08180973, -0.04178543],\n",
       "       [-3.6323197 ,  3.91172   ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = tf_model(tf_batch)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amber-criminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(tf_outputs))\n",
    "len(tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "indonesian-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-4.0832963 ,  4.336415  ],\n",
       "       [ 0.08180973, -0.04178543],\n",
       "       [-3.6323197 ,  3.91172   ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final activations\n",
    "# (n_samples, n_class)\n",
    "\n",
    "print(type(tf_outputs[0]))\n",
    "tf_outputs[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "departmental-major",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "<ipython-input-29-33457bdf00ea> in <module>\n",
    "----> 1 tf_outputs[1]\n",
    "\n",
    "~/anaconda3/envs/tf2.4/lib/python3.7/site-packages/transformers/file_utils.py in __getitem__(self, k)\n",
    "   1446             return inner_dict[k]\n",
    "   1447         else:\n",
    "-> 1448             return self.to_tuple()[k]\n",
    "   1449 \n",
    "   1450     def __setattr__(self, name, value):\n",
    "\n",
    "IndexError: tuple index out of range\n",
    "\"\"\"\n",
    "\n",
    "tf_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decimal-belief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[-4.0832963 ,  4.336415  ],\n",
      "       [ 0.08180973, -0.04178543],\n",
      "       [-3.6323197 ,  3.91172   ]], dtype=float32)>, hidden_states=None, attentions=None)\n",
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[-4.0832963 ,  4.336415  ],\n",
      "       [ 0.08180973, -0.04178543],\n",
      "       [-3.6323197 ,  3.91172   ]], dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(repr(tf_outputs))\n",
    "print(tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "played-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[2.2042972e-04, 9.9977952e-01],\n",
       "       [5.3085953e-01, 4.6914047e-01],\n",
       "       [5.2897527e-04, 9.9947101e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-referral",
   "metadata": {},
   "source": [
    "#### predict loss + activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "antique-angel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.2051287e-04, 6.3325787e-01, 5.2914920e-04], dtype=float32)>, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-4.0832963 ,  4.336415  ],\n",
       "       [ 0.08180973, -0.04178543],\n",
       "       [-3.6323197 ,  3.91172   ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params: labels, means y_true, for test only\n",
    "\n",
    "# tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))\n",
    "# tf_outputs = tf_model(tf_batch, labels = tf.constant([2, 1, 0]))\n",
    "tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0, 1]))\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "private-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(tf_outputs))\n",
    "len(tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "numerical-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.2051287e-04, 6.3325787e-01, 5.2914920e-04], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "# (n_samples, )\n",
    "\n",
    "print(type(tf_outputs[0]))\n",
    "tf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "apart-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-4.0832963 ,  4.336415  ],\n",
       "       [ 0.08180973, -0.04178543],\n",
       "       [-3.6323197 ,  3.91172   ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final activations\n",
    "# (n_samples, n_class)\n",
    "\n",
    "print(type(tf_outputs[1]))\n",
    "tf_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cutting-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[2.2042972e-04, 9.9977952e-01],\n",
       "       [5.3085953e-01, 4.6914047e-01],\n",
       "       [5.2897527e-04, 9.9947101e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape, axis 0, 1\n",
    "# same as numpy, axis=-1, mean 1 at this place, which means row\n",
    "# do softmax on each row\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[1], axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-lease",
   "metadata": {},
   "source": [
    "## Without AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-twins",
   "metadata": {},
   "source": [
    "### Tokenizer + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "democratic-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "vocal-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "disabled-oxygen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "amino-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_77']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "informed-convertible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-4.0832963 ,  4.336415  ],\n",
       "       [ 0.08180973, -0.04178543],\n",
       "       [-3.6323197 ,  3.91172   ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = model(tf_batch)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-dylan",
   "metadata": {},
   "source": [
    "### Customize by config: Tokenizer + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "preliminary-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "manual-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "authorized-inside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "desirable-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scratch\n",
    "model = TFDistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "spatial-institution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.09060648,  0.01755198],\n",
       "       [-0.09387132,  0.02144592],\n",
       "       [-0.0976468 ,  0.04282071]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = model(tf_batch)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-brush",
   "metadata": {},
   "source": [
    "### Customize by param: Tokenizer + Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-villa",
   "metadata": {},
   "source": [
    "TODO: how to judge wheather train model from scratch is needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acute-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "running-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "valid-message",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "herbal-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_117']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "gorgeous-berry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(3, 10), dtype=float32, numpy=\n",
       "array([[-0.06570223, -0.07684831,  0.00491758,  0.01898364, -0.02881462,\n",
       "        -0.00743424,  0.03098125,  0.00872145,  0.09670264,  0.07677424],\n",
       "       [-0.03626308, -0.10396606,  0.0107096 , -0.00623686, -0.05597301,\n",
       "         0.03577782,  0.02595923,  0.03559294,  0.11004325,  0.08273635],\n",
       "       [ 0.0102365 , -0.0793872 , -0.01624559, -0.00732068, -0.03361801,\n",
       "         0.01050154,  0.023844  ,  0.03455131,  0.12689522,  0.07237536]],\n",
       "      dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = model(tf_batch)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-bubble",
   "metadata": {},
   "source": [
    "## Ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-value",
   "metadata": {},
   "source": [
    "[1] https://huggingface.co/transformers/quicktour.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-hungary",
   "metadata": {},
   "source": [
    "when import model with  pyTorch only to TF, the pyTorch library is needed <br>\n",
    "[2] https://github.com/huggingface/transformers/issues/7138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
