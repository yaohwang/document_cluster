{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excited-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educational-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from itertools import chain\n",
    "from itertools import zip_longest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handy-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-12 11:07:11 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2021-04-12 11:07:12 DEBUG https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2021-04-12 11:07:12 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2021-04-12 11:07:13 DEBUG https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/main/sentencepiece.bpe.model HTTP/1.1\" 200 0\n",
      "2021-04-12 11:07:13 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2021-04-12 11:07:13 DEBUG https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/main/tokenizer.json HTTP/1.1\" 200 0\n",
      "Building model \u001b[5m\u001b[33m...\u001b[0m\u001b[0m2021-04-12 11:07:14 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2021-04-12 11:07:15 DEBUG https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2021-04-12 11:07:15 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2021-04-12 11:07:16 DEBUG https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Moving model to GPUs [0] \u001b[5m\u001b[33m...\u001b[0m\u001b[                          \r"
     ]
    }
   ],
   "source": [
    "from mih import VocabularyBPE\n",
    "from mih import TokenizerBPE\n",
    "# from preprocess_v2 import NormalizerSGZChat\n",
    "from preprocess_v3 import NormalizerSGZChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hazardous-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "(14686, 2)\n",
      "34686 documents - 1.595MB (training set)\n",
      "1017 documents - 0.061MB (test set)\n",
      "\n",
      "Stored 'data_train' (DataFrame)\n",
      "Stored 'X_train' (Series)\n",
      "Stored 'X_test' (Series)\n",
      "Stored 'y_train' (list)\n",
      "Stored 'y_test' (list)\n",
      "Stored 'data_train_size_mb' (float)\n"
     ]
    }
   ],
   "source": [
    "%run dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-telephone",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "declared-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_train\n",
    "%store -r X_train\n",
    "%store -r data_train_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-liabilities",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "apparent-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_special(i):\n",
    "    \n",
    "    is_1 =  i in ['<unk>', '<loc>', '<contact>', '<recruit>', '<corpus>', '<colonel>']\n",
    "    is_2 = bool(re.match(r'^<num-[0-9]+>$', i))\n",
    "    \n",
    "    return is_1 or is_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "important-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_special(i):\n",
    "    return not is_special(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-paradise",
   "metadata": {},
   "source": [
    "### replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "further-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __replace(s):\n",
    "    if is_special(s): return s\n",
    "    \n",
    "    # 微信\n",
    "    s = s.replace('威', '微')\n",
    "    s = s.replace('徽', '微')\n",
    "    s = s.replace('徵', '微')\n",
    "    s = s.replace('亻言', '信')\n",
    "    \n",
    "    s = s.replace('微新', '微信')\n",
    "    s = s.replace('微信', '微')\n",
    "    \n",
    "    # 加\n",
    "    s = s.replace('咖', '加')\n",
    "    s = s.replace('架', '加')\n",
    "    s = s.replace('嫁', '加')\n",
    "    s = s.replace('十', '加')\n",
    "    s = s.replace('茄', '加')\n",
    "    s = s.replace('迦', '加')\n",
    "    \n",
    "    s = s.replace('加下', '加')\n",
    "    s = s.replace('加一下', '加')\n",
    "    \n",
    "    s = s.replace('加', '+')\n",
    "    \n",
    "    # 收人\n",
    "    s = s.replace('活人', '人')\n",
    "    # s = s.replace('收人', '<recruit>')\n",
    "    \n",
    "    # 团长\n",
    "    s = s.replace('圕', '团')\n",
    "    \n",
    "    # 充\n",
    "    s = s.replace('冲', '充')\n",
    "    s = s.replace('直充', '充')\n",
    "    \n",
    "    # 出\n",
    "    s = s.replace('础', '出')\n",
    "\n",
    "    # 卖\n",
    "    s = s.replace('麦', '出')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atlantic-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(x):\n",
    "    return [__replace(i) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-excellence",
   "metadata": {},
   "source": [
    "### split util"
   ]
  },
  {
   "cell_type": "raw",
   "id": "naughty-citation",
   "metadata": {},
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "later-spell",
   "metadata": {},
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ambient-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_regex(s, reg, flag):\n",
    "    r = re.split(reg, s)\n",
    "    if 1 >= len(r): return r\n",
    "    r = list(chain.from_iterable(zip_longest(r[:-1], [], fillvalue=flag))) + r[-1:]\n",
    "    return [i for i in r if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-exclusive",
   "metadata": {},
   "source": [
    "#### split location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coordinate-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(s):\n",
    "    return split_regex(s, r'{localization:[0-9]+\\-[0-9]+}', '<loc>')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "european-blackjack",
   "metadata": {},
   "source": [
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」{localization:189-393}，889'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}，88965」'))\n",
    "print(split_location('「开仓吃饭」「{localization:189-393}'))\n",
    "print(split_location('{localization:189-393}，88965」'))\n",
    "print(split_location('{localization:189-393}'))\n",
    "print(split_location('2{l4ocalization:189-393}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-disposition",
   "metadata": {},
   "source": [
    "#### split terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "guilty-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_terminology(s, term, flag):\n",
    "    if is_special(s): return [s]\n",
    "    return split_regex(s, r'%s' % term, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "practical-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_terminology(x, term, flag):\n",
    "    return list(chain.from_iterable([__split_terminology(i, term, flag) for i in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-instruction",
   "metadata": {},
   "source": [
    "#### split coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sporting-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-fifth",
   "metadata": {},
   "source": [
    "#### split num + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "copyrighted-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_charnum(s):\n",
    "    if is_special(s): return [s]\n",
    "    return [c for c in re.split(r'([0-9a-z]+)', s) if c]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "talented-oxide",
   "metadata": {},
   "source": [
    "print(__split_charnum('yweighu768980k上jwio880中不为'))\n",
    "print(__split_charnum('上中不为'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hybrid-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_charnum(x):\n",
    "    return list(chain.from_iterable([__split_charnum(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-rotation",
   "metadata": {},
   "source": [
    "#### convert num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlikely-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^0-9]+', s))\n",
    "    \n",
    "    return has_num and hasnot_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opposed-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    # return ['<num-%s>' % len(i) if i.isnumeric() else i for i in x]\n",
    "    return ['<num-%s>' % len(i) if not_special(i) and is_numeric(i) else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-truth",
   "metadata": {},
   "source": [
    "#### convert num + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_charnum(s):\n",
    "    \n",
    "    has_num = bool(re.findall(r'[0-9]+', s))\n",
    "    has_char = bool(re.findall(r'[a-z]+', s))\n",
    "    hasnot_other = not bool(re.findall(r'[^a-z0-9]+', s))\n",
    "    \n",
    "    return has_num and has_char and hasnot_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alike-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_v_num(s):\n",
    "    return bool(re.match(r'v[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "racial-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vx_num(s):\n",
    "    return bool(re.match(r'vx[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "possible-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_qq_num(s):\n",
    "    return bool(re.match(r'qq[0-9]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "opposed-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __convert_chars_numbers(s):\n",
    "    \n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    if is_v_num(s): return ['微', '<contact>']\n",
    "    if is_vx_num(s): return ['微', '<contact>']\n",
    "    if is_qq_num(s): return ['微', '<contact>']\n",
    "    \n",
    "    if is_charnum(s): return ['<contact>']\n",
    "    \n",
    "    return [s]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fixed-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_chars_numbers(x):\n",
    "    # return ['<contact>' if not_special(i) and is_charnum(i) else i for i in x]\n",
    "    return list(chain.from_iterable([__convert_chars_numbers(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-positive",
   "metadata": {},
   "source": [
    "#### split char, num, chinese + special"
   ]
  },
  {
   "cell_type": "raw",
   "id": "greatest-trouble",
   "metadata": {},
   "source": [
    "s = '◢好◣oooo ┏━卖资源━┓ oooo◢天◣ ◥好◤( 乖)┃高◣┃◢功┃(乖 )◥天◤ ◢游◣ * ( ┃迁◤┃◥勋┃ ) * ◢资◣ ◥戏◤ *__)┗━じovの━┛(__* ◥源◤ __________微信jinyanzifei8__________'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tired-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_normal_special(s):\n",
    "    if is_special(s): return s, ''\n",
    "    \n",
    "    # TODO: wheather , . ，。blank should be in valid\n",
    "    return ''.join(re.findall(r'[,\\+a-z0-9\\u4e00-\\u9fa5]+', s)), ''.join(re.findall(r'[^,\\+a-z0-9\\u4e00-\\u9fa5]+', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nervous-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_normal_special(x):\n",
    "    \n",
    "    r = [__split_normal_special(i) for i in x]\n",
    "    \n",
    "    r1, r2 = zip(*r)\n",
    "    r1 = [i for i in r1 if i]\n",
    "    r2 = [i for i in r2 if i]\n",
    "    \n",
    "    return r1, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-detective",
   "metadata": {},
   "source": [
    "#### split naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "general-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_naive(s):\n",
    "    # return jieba.lcut(s)\n",
    "    # return jieba.lcut(s, cut_all=True)\n",
    "    # return jieba.lcut_for_search(s)\n",
    "    return list(s)\n",
    "    \n",
    "    # return jieba.lcut(s, cut_all=True) + list(s)\n",
    "    # return jieba.lcut_for_search(s) + list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-toner",
   "metadata": {},
   "source": [
    "#### split once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "color-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __split_once(s):\n",
    "    if is_special(s): return [s]\n",
    "    \n",
    "    # s1, s2 = split_normal_special(s)\n",
    "    \n",
    "    # return __split0(s1) + __split0(s2)\n",
    "    # return split_naive(s1) + list(s2)\n",
    "    \n",
    "    # r = split_naive(s1) + list(s)\n",
    "    \n",
    "    # r1 = split_naive(s1)\n",
    "    # r2 = list(s)\n",
    "    # r = r1 + list(set(r2)-set(r1))\n",
    "    \n",
    "    # s = replace(s)\n",
    "    # r = list(s)\n",
    "    r = split_naive(s)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "opposite-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_once(x):\n",
    "    return list(chain.from_iterable([__split_once(s) for s in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-quantity",
   "metadata": {},
   "source": [
    "#### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "elementary-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1_usual = ['你', '我', '他', '她', '它', '们',\n",
    "                    '吧', '吗', '嘛', '啊', '阿', '呢', '呀',\n",
    "                    '的', '地', \n",
    "                    '怎', '么',\n",
    "                    '那', '哪',\n",
    "                    '就', '没', '了', '谢', '配', '合']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "operational-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1 = stopwords1_usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "portable-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "several-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(x):\n",
    "    return [i for i in x if i not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-career",
   "metadata": {},
   "source": [
    "### split 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "turkish-greece",
   "metadata": {},
   "source": [
    "def __split(s):\n",
    "    r = []\n",
    "    l = re.split(loc, s)\n",
    "    \n",
    "    for i in l[:-1]:\n",
    "        r.extend(__split1(i))\n",
    "        r.append('<loc>')\n",
    "        \n",
    "    r.extend(__split1(l[-1]))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "id": "extraordinary-allah",
   "metadata": {},
   "source": [
    "def split1(s):\n",
    "    s = s.lower()\n",
    "    # tokens = jieba.lcut(s, cut_all=True)\n",
    "    # tokens = __split(s)\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # tokens = split_chars_numbers(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    tokens = convert_num(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "gothic-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split1(s):\n",
    "    # preprocess\n",
    "    s = s.replace(' ', '')    # TODO: maybe all blank\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    \n",
    "    # formated\n",
    "    tokens = split_location(s)\n",
    "    \n",
    "    # normal and special-char\n",
    "    tokens, tokens_special = split_normal_special(tokens)\n",
    "    \n",
    "    # user defined\n",
    "    tokens = split_charnum(tokens)\n",
    "    tokens = convert_num(tokens)\n",
    "    tokens = convert_chars_numbers(tokens)\n",
    "    \n",
    "    # link\n",
    "    tokens = replace(tokens)\n",
    "    \n",
    "    # split\n",
    "    tokens = split_terminology(tokens, '收人', '<recruit>')\n",
    "    tokens = split_terminology(tokens, '军团', '<corpus>')\n",
    "    tokens = split_terminology(tokens, '团长', '<colonel>')\n",
    "    tokens = split_once(tokens)\n",
    "    \n",
    "    # filter\n",
    "    tokens = filter_stopwords(tokens)\n",
    "    \n",
    "    # merge tokens_special\n",
    "    # tokens += list(''.join(tokens_special))\n",
    "    tokens += ['<special-char>'] * len(''.join(tokens_special))\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "textile-bouquet",
   "metadata": {},
   "source": [
    "# print(split1('有资源，有功勋 {localization:356-290}'))\n",
    "# print(split1('林昊天 有事找， 你架下他徽 a a z 0 1 2 4'))\n",
    "# print(split1('－－大家看角色名字，需要加ｑ７６６－６４５－８５１菿－付'))\n",
    "print(split1('军团收活人，来的加 1979574312私聊'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fifth-entertainment",
   "metadata": {},
   "source": [
    "split1 = list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-paris",
   "metadata": {},
   "source": [
    "### high freq"
   ]
  },
  {
   "cell_type": "raw",
   "id": "framed-habitat",
   "metadata": {},
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-2, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "infinite-fight",
   "metadata": {},
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sealed-effort",
   "metadata": {},
   "source": [
    "def high_freq(x):\n",
    "    return [i for i in x if is_special(i) or (i not in feature_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-heading",
   "metadata": {},
   "source": [
    "### low freq"
   ]
  },
  {
   "cell_type": "raw",
   "id": "norman-subsection",
   "metadata": {},
   "source": [
    "vocab = VocabularyBPE()\n",
    "# vocab.load_vocab('./.vocab/.checkpoint-last-15000')\n",
    "vocab.load_vocab('./.vocab/.checkpoint-last-20000')\n",
    "tokenizer = TokenizerBPE(vocab, load_chars=False)\n",
    "\n",
    "# split1 = lambda x: tokenizer.tokenize([NormalizerSGZChat()(x)])[0]\n",
    "# split1 = lambda x: NormalizerSGZChat()(x)\n",
    "split1 = lambda x: filter_stopwords(NormalizerSGZChat()(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dressed-minister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 3.046775s at 0.523MB/s\n",
      "n_samples: 34686, n_features: 1046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1e-3, tokenizer=split1)\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "        \n",
    "duration = time() - t0\n",
    "\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fifteen-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "herbal-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./model/ads-detect-1-20210125.vocab', 'wb') as f:\n",
    "# with open('./model/ads-detect-1-20210312.vocab', 'wb') as f:\n",
    "with open('./model/ads-detect-1-20210401.vocab', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fossil-dollar",
   "metadata": {},
   "source": [
    "for f in feature_names: print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "comfortable-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_freq(x):\n",
    "    return ['<unk>' if i not in feature_names else i for i in x]\n",
    "    # return ['[UNK]' if i not in feature_names else i for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-printer",
   "metadata": {},
   "source": [
    "### split 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "chemical-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2(s):\n",
    "    # return low_freq(split1(s))\n",
    "\n",
    "    tokens = split1(s)\n",
    "    # tokens = high_freq(tokens)\n",
    "    tokens = low_freq(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-longer",
   "metadata": {},
   "source": [
    "### split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "primary-weekend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [<unk>, <unk>, <unk>, <contact>]\n",
       "1    [九, ,, <unk>, <unk>, 团, 里, 有, <unk>, 之, 妇, ,, ...\n",
       "2    [<contact>, 元, 宝, +, <contact>, +, 武, 将, +, 各,...\n",
       "3                [九, ,, 吃, 里, <unk>, 外, ,, 结, 义, 内, 鬼]\n",
       "4    [资, 源, 功, 勋, 高, 迁, 微, <contact>, 是, 小, <unk>, ...\n",
       "5    [收, 魏, 国, 高, 迁, 号, <special-char>, <special-ch...\n",
       "6                      [出, 资, 源, 有, 需, 要, +, <num-11>]\n",
       "7    [微, <contact>, 以, 下, 刷, 功, 勋, 私, 聊, 公, 平, 刷, ,...\n",
       "8    [带, 拉, 量, <unk>, 大, ,, 高, 迁, 功, 勋, ,, 大, 量, 低,...\n",
       "9    [带, 拉, 量, <unk>, 大, ,, 高, 迁, 功, 勋, ,, 大, 量, 低,...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['content'].head(10).apply(split2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
